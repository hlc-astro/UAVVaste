# Load custom dataset
custom_dataset = CustomDataset(root_dir=path_to_images, json_path=path_to_json)

# Create a DataLoader
dataloader = DataLoader(custom_dataset, batch_size=4, shuffle=True)

# Fine-tuning the model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
sam_model.to(device)
sam_model.train()

# Set up optimizer
optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters())

# Set up loss function
loss_fn = torch.nn.MSELoss()

# Training loop
num_epochs = 2
for epoch in range(num_epochs):
    for batch in dataloader:
        input_image, gt_binary_mask, bboxes = batch['image'], batch['masks'], batch['bboxes']
        input_image, gt_binary_mask = input_image.to(device), gt_binary_mask.to(device)

        # Image encoding
        with torch.no_grad():
            image_embedding = sam_model.image_encoder(input_image)

        # Prompt encoding
        with torch.no_grad():
            # Update this part based on your prompt encoder usage
            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(points=None, boxes=bboxes, masks=None)

        # Mask decoding
        low_res_masks, iou_predictions = sam_model.mask_decoder(
            image_embeddings=image_embedding,
            image_pe=sam_model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )

        # Postprocessing
        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)

        # Generate binary mask
        binary_mask = F.normalize(F.threshold(upscaled_masks, 0.0, 0)).to(device)

        # Calculate loss and backpropagate
        loss = loss_fn(binary_mask, gt_binary_mask)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Save the fine-tuned model
#torch.save(sam_model.state_dict(), 'fine_tuned_sam.pth')

# Create the reduced annotations data with train images and test annotations
train_reduced_annotations_data = {
    "images": train_images_info,
    "categories": train_categories,
    "annotations": train_annotations
}

# Save the reduced_annotations_data as a JSON file in a COCO-like structure with indentation
with open('train_reduced_annotations.json', 'w') as reduced_annotations_file:
    json.dump(train_reduced_annotations_data, reduced_annotations_file, indent=2)


# Create the reduced annotations data with val images and test annotations
val_reduced_annotations_data = {
    "images": val_images_info,
    "categories": val_categories,
    "annotations": val_annotations
}

# Save the reduced_annotations_data as a JSON file in a COCO-like structure with indentation
with open('val_reduced_annotations.json', 'w') as reduced_annotations_file:
    json.dump(val_reduced_annotations_data, reduced_annotations_file, indent=2)